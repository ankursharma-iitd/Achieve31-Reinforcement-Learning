{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for vectors manipulation\n",
    "import numpy as np\n",
    "\n",
    "# for plotting\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "# visualize plots in the jupyter notebook\n",
    "# check more https://goo.gl/U3Ai8R\n",
    "%matplotlib inline\n",
    "\n",
    "# for generating random values\n",
    "import random\n",
    "\n",
    "# for representing things like card value or colors\n",
    "from enum import Enum  \n",
    "\n",
    "# for copying python objects\n",
    "import copy\n",
    "\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "\n",
    "from pylab import rcParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method adapted from Easy21 assignment of https://github.com/analog-rl/Easy21\n",
    "def plot_value_function(agent_type, value_function, method, title='Value Function', usables=[0,0,0], generate_gif=False, train_steps=None, save=None, transpose=True):\n",
    "    \"\"\"\n",
    "    Plots a value function as a surface plot, like in: https://goo.gl/aF2doj\n",
    "\n",
    "    You can choose between just plotting the graph for the value function\n",
    "    which is the default behaviour (generate_gif=False) or to train the agent\n",
    "    a couple of times and save the frames in a gif as you train.\n",
    "\n",
    "    Args:\n",
    "        agent: An agent.\n",
    "        title (string): Plot title.\n",
    "        generate_gif (boolean): If want to save plots as a gif.\n",
    "        train_steps: If is not None and generate_gif = True, then will use this\n",
    "                     value as the number of steps to train the model at each frame.\n",
    "    \"\"\"\n",
    "    # you can change this values to change the size of the graph\n",
    "    title += ' (' + str(train_steps) + ' Episodes, Usables ' + str(usables) + ', ' + method + ')'\n",
    "    title = agent_type + ' ' + title\n",
    "    fig = plt.figure(title, figsize=(10, 5))\n",
    "    \n",
    "    # explanation about this line: https://goo.gl/LH5E7i\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    if transpose:\n",
    "        V = np.transpose(value_function[:,:,usables[0],usables[1],usables[2]])\n",
    "    else:\n",
    "        V = value_function[:,:,usables[0],usables[1],usables[2]]\n",
    "    \n",
    "    if generate_gif:\n",
    "        print('gif will be saved as %s' % title)\n",
    "    \n",
    "    def plot_frame(ax):\n",
    "        # min value allowed accordingly with the documentation is 1\n",
    "        # we're getting the max value from V dimensions\n",
    "        min_x = 1\n",
    "        max_x = V.shape[0]\n",
    "        min_y = 1\n",
    "        max_y = V.shape[1]\n",
    "\n",
    "        # creates a sequence from min to max\n",
    "        x_range = np.arange(min_x, max_x)\n",
    "        y_range = np.arange(min_y, max_y)\n",
    "\n",
    "        # creates a grid representation of x_range and y_range\n",
    "        X, Y = np.meshgrid(x_range, y_range)\n",
    "\n",
    "        # get value function for X and Y values\n",
    "        def get_stat_val(x, y):\n",
    "            return V[x, y]\n",
    "        Z = get_stat_val(X, Y)\n",
    "\n",
    "        # creates a surface to be ploted\n",
    "        # check documentation for details: https://goo.gl/etEhPP\n",
    "        ax.set_xlabel('Dealer Showing')\n",
    "        ax.set_ylabel('Player Sum')\n",
    "        ax.set_zlabel('Value')\n",
    "        return ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=cm.coolwarm, \n",
    "                               linewidth=0, antialiased=False)\n",
    "    \n",
    "    def animate(frame):\n",
    "        # clear the plot and create a new surface\n",
    "        ax.clear()\n",
    "        surf = plot_frame(ax)\n",
    "        # if we're going to generate a gif we need to train a couple of times\n",
    "        if generate_gif:\n",
    "            i = agent.iterations\n",
    "            # cool math to increase number of steps as we go\n",
    "            if train_steps is None:\n",
    "                step_size = int(min(max(1, agent.iterations), 2 ** 16))\n",
    "            else:\n",
    "                step_size = train_steps\n",
    "\n",
    "            agent.train(step_size)\n",
    "            plt.title('%s MC score: %s frame: %s' % (title, float(agent.wins)/agent.iterations*100, frame))\n",
    "        else:\n",
    "            plt.title(title)\n",
    "\n",
    "        fig.canvas.draw()\n",
    "        return surf\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, animate, 32, repeat=False)\n",
    "\n",
    "    # requires gif writer\n",
    "    if generate_gif:\n",
    "        ani.save(title + '.gif', writer='imagemagick', fps=3)\n",
    "    else:\n",
    "        if save is None:\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.savefig(save)\n",
    "            plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Class for Color, Action, Card, Deck, Player, Dealer, State, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Color(Enum):\n",
    "    BLACK = 0\n",
    "    RED = 1\n",
    "    \n",
    "class Action(Enum):\n",
    "    HIT = 0\n",
    "    STICK = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Card(object):\n",
    "    def __init__(self, color=None, value=None):\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "        else:\n",
    "            self.value = random.randint(1, 10)\n",
    "        if color == Color.BLACK or color == Color.RED:\n",
    "            self.color = color\n",
    "        else:\n",
    "            random_num = random.random()\n",
    "            if(random_num <= (1.0/3)):\n",
    "                self.color = Color.RED\n",
    "            else:\n",
    "                self.color = Color.BLACK\n",
    "                \n",
    "    def _print_card_(self):\n",
    "        print(\"Color: {}, Value: {}\".format(self.color, self.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deck(object):\n",
    "    def sample_card(self, color=None, value=None):\n",
    "        return Card(color, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State characterised by (agent_sum, dealer_sum, Usable1, Usable2, Usable3)\n",
    "# Usable1 = 0 -> there is no usable 1 in the current hand\n",
    "class State(object):\n",
    "    def __init__(self, agent_sum=0, show_card=0, agent_usables=[0,0,0], dealer_sum=0, dealer_usables=[0,0,0], is_terminal=False):\n",
    "        self.show_card = show_card\n",
    "        self.agent_sum = agent_sum\n",
    "        self.dealer_sum = dealer_sum\n",
    "        self.agent_usables = agent_usables\n",
    "        self.dealer_usables = dealer_usables\n",
    "        self.is_terminal = is_terminal\n",
    "        \n",
    "    def get_tuple(self):\n",
    "        usables = self.agent_usables\n",
    "        return (self.agent_sum, self.show_card - 4, usables[0], usables[1], usables[2])\n",
    "        \n",
    "    def _print_state_(self):\n",
    "        print(\"Dealer-Face-Card: ({}), Agent-Sum: ({}), Dealer-Sum: ({}), Usables: ({}, {}), Terminal: ({})\".format(\n",
    "                                                                                            self.show_card,\\\n",
    "                                                                                            self.agent_sum,\\\n",
    "                                                                                            self.dealer_sum,\\\n",
    "                                                                                            self.agent_usables,\\\n",
    "                                                                                            self.dealer_usables,\\\n",
    "                                                                                            self.is_terminal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player(object):\n",
    "    def policy(self, state):\n",
    "        raise NotImplemented()\n",
    "\n",
    "class Dealer(object):\n",
    "    def policy(self, state):\n",
    "        if(state.dealer_sum >= 25):\n",
    "            return Action.STICK\n",
    "        else:\n",
    "            return Action.HIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Environment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    def __init__(self):\n",
    "        self.deck = Deck()\n",
    "        self.dealer = Dealer()\n",
    "        self.agent_max_value = 32  # max value an agent can get during the game\n",
    "        self.dealer_max_value = 10  # max value the dealer can get when taking the first card\n",
    "        self.usable_1_values = 3 # can take states 0(not-present), 1(usable) or 2(non-usable) for card-value 1\n",
    "        self.usable_2_values = 3 # can take states 0(not-present), 1(usable) or 2(non-usable) for card-value 2\n",
    "        self.usable_3_values = 3 # can take states 0(not-present), 1(usable) or 2(non-usable) for card-value 3\n",
    "        self.actions_count = 2  # number of possible actions in each state\n",
    "        \n",
    "    def check_bust(self, current_sum):\n",
    "        return (current_sum < 0) or (current_sum > 31)\n",
    "    \n",
    "    # get reward when both of them have sticked without going bust\n",
    "    def get_reward_bust(self, state):\n",
    "        if(state.agent_sum > state.dealer_sum):\n",
    "            return 1\n",
    "        elif(state.agent_sum == state.dealer_sum):\n",
    "            return 0\n",
    "        return -1\n",
    "    \n",
    "    def sample_card_value(self, color=None, value=None):\n",
    "        card = self.deck.sample_card(color, value)\n",
    "        if(card.color == Color.BLACK):\n",
    "            return card.value\n",
    "        else:\n",
    "            return -card.value\n",
    "    \n",
    "    def update_dealer_usables(self, state, card_value):\n",
    "        if(card_value == 1 or card_value == 2 or card_value == 3):\n",
    "            idx = card_value - 1\n",
    "            old_state = state.dealer_usables[idx]\n",
    "            if(old_state == 0):\n",
    "                # previously not present\n",
    "                # value can either be new_value or (new_value + 10)\n",
    "                if self.check_bust(state.dealer_sum + card_value + 10):\n",
    "                    # going bust when using (new_value + 10)\n",
    "                    # make it non-usable\n",
    "                    new_state = 2\n",
    "                else:\n",
    "                    # can use it as a usable card, add 10 and make it usable\n",
    "                    card_value += 10\n",
    "                    new_state = 1\n",
    "            elif(old_state == 1):\n",
    "                # previously used as a usable\n",
    "                new_state = 2\n",
    "            else:\n",
    "                new_state = 2\n",
    "            state.dealer_usables[idx] = new_state\n",
    "        return state, card_value\n",
    "    \n",
    "    def update_agent_usables(self, state, card_value):\n",
    "        if(card_value == 1 or card_value == 2 or card_value == 3):\n",
    "            idx = card_value - 1\n",
    "            old_state = state.agent_usables[idx]\n",
    "            if(old_state == 0):\n",
    "                # previously not present\n",
    "                # value can either be new_value or (new_value + 10)\n",
    "                if self.check_bust(state.agent_sum + card_value + 10):\n",
    "                    # going bust when using (new_value + 10)\n",
    "                    # make it non-usable\n",
    "                    new_state = 2\n",
    "                else:\n",
    "                    # can use it as a usable card, add 10 and make it usable\n",
    "                    card_value += 10\n",
    "                    new_state = 1\n",
    "            elif(old_state == 1):\n",
    "                # previously used as a usable\n",
    "                new_state = 2\n",
    "            else:\n",
    "                new_state = 2\n",
    "            state.agent_usables[idx] = new_state\n",
    "        return state, card_value\n",
    "        \n",
    "    # play dealer's turn when the agent has sticked    \n",
    "    def play_dealer(self, state):\n",
    "        while(True):\n",
    "            action = self.dealer.policy(state)\n",
    "            if(action == Action.HIT):\n",
    "                new_card_value = self.sample_card_value()\n",
    "                state, new_card_value = self.update_dealer_usables(copy.deepcopy(state), new_card_value)\n",
    "                state.dealer_sum += new_card_value\n",
    "            state.is_terminal = self.check_bust(state.dealer_sum)\n",
    "            if(state.is_terminal or (action == Action.STICK)):\n",
    "                break\n",
    "        return state\n",
    "        \n",
    "    # Both the dealer and agent take one card each, dealer's card is visible to the agent\n",
    "    def initial_state(self):\n",
    "        agent_card_value = self.sample_card_value()\n",
    "        dealer_card_value = self.sample_card_value()\n",
    "        state = State(agent_usables=[0,0,0], dealer_usables=[0,0,0])\n",
    "        state, agent_card_value = self.update_agent_usables(copy.deepcopy(state), agent_card_value)\n",
    "        state, dealer_card_value = self.update_dealer_usables(copy.deepcopy(state), dealer_card_value)\n",
    "        state.agent_sum += agent_card_value\n",
    "        state.dealer_sum += dealer_card_value\n",
    "        state.show_card = dealer_card_value\n",
    "        agent_busted = self.check_bust(state.agent_sum)\n",
    "        dealer_busted = self.check_bust(state.show_card)\n",
    "        if (agent_busted or dealer_busted):\n",
    "            state.is_terminal = True\n",
    "        else:\n",
    "            state.is_terminal = False\n",
    "        return state\n",
    "    \n",
    "    # Given (state, action) return (next_state, reward)\n",
    "    def step(self, state, action):\n",
    "        agent_sum = state.agent_sum\n",
    "        show_card = state.show_card\n",
    "        agent_usables = state.agent_usables\n",
    "        reward = 0\n",
    "        next_state = copy.deepcopy(state)\n",
    "        if(state.is_terminal):\n",
    "            print('Cannot take action on a terminal state')\n",
    "        if action == Action.STICK:\n",
    "            next_state = self.play_dealer(copy.deepcopy(state))\n",
    "            if next_state.is_terminal:\n",
    "                reward = 1\n",
    "            else:\n",
    "                next_state.is_terminal = True\n",
    "                reward = self.get_reward_bust(next_state)\n",
    "        else:\n",
    "            agent_card_value = self.sample_card_value()\n",
    "            next_state, agent_card_value = self.update_agent_usables(copy.deepcopy(state), agent_card_value)\n",
    "            next_state.agent_sum += agent_card_value\n",
    "            next_state.is_terminal = self.check_bust(next_state.agent_sum)\n",
    "            if next_state.is_terminal:\n",
    "                reward = -1\n",
    "        return next_state, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Agent (Base Class for all Agents Learnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Player):\n",
    "    def __init__(self, environment, discount_factor=1.0):\n",
    "        Player.__init__(self)\n",
    "        self.env = environment\n",
    "        self.discount_factor = discount_factor\n",
    "        self.value_function = np.zeros([self.env.agent_max_value, \\\n",
    "                                        self.env.dealer_max_value, \\\n",
    "                                        self.env.usable_1_values, \\\n",
    "                                        self.env.usable_2_values, \\\n",
    "                                        self.env.usable_3_values])\n",
    "        self.action_value_function = np.zeros([self.env.agent_max_value, \\\n",
    "                                              self.env.dealer_max_value, \\\n",
    "                                              self.env.usable_1_values, \\\n",
    "                                              self.env.usable_2_values, \\\n",
    "                                              self.env.usable_3_values, \\\n",
    "                                              self.env.actions_count])\n",
    "        self.matches_won = 0.0\n",
    "        self.matches_draw = 0.0\n",
    "        self.matches_lose = 0.0\n",
    "        self.num_games_played = 0.0\n",
    "    \n",
    "    def get_value_function(self):\n",
    "        for i in range(self.env.agent_max_value):\n",
    "            for j in range(self.env.dealer_max_value):\n",
    "                for k in range(self.env.usable_1_values):\n",
    "                    for l in range(self.env.usable_2_values):\n",
    "                        for m in range(self.env.usable_3_values):\n",
    "                            s = State(i,j+4,[k,l,m])\n",
    "                            action = self.take_greedy_action(s)\n",
    "                            satup = s.get_tuple() + (action.value, )\n",
    "                            self.value_function[s.get_tuple()] = self.action_value_function[satup]\n",
    "        return self.value_function\n",
    "    \n",
    "    def get_action_value_function(self):\n",
    "        return self.action_value_function\n",
    "        \n",
    "    def take_random_action(self):\n",
    "        num = random.random()\n",
    "        if num <= 0.5:\n",
    "            return Action.HIT\n",
    "        return Action.STICK\n",
    "    \n",
    "    def take_greedy_action(self, state):\n",
    "        stup = state.get_tuple()\n",
    "        Q = self.get_action_value_function()\n",
    "        return Action(np.argmax(Q[stup]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAgentEvaluation(Agent):\n",
    "    def __init__(self, environment, discount_factor=1):\n",
    "        Agent.__init__(self, environment, discount_factor)\n",
    "        self.visit_count_V = np.zeros([self.env.agent_max_value, \\\n",
    "                            self.env.dealer_max_value, \\\n",
    "                            self.env.usable_1_values, \\\n",
    "                            self.env.usable_2_values, \n",
    "                            self.env.usable_3_values])\n",
    "        self.visit_count_Q = np.zeros([self.env.agent_max_value, \\\n",
    "                           self.env.dealer_max_value, \\\n",
    "                           self.env.usable_1_values, \\\n",
    "                           self.env.usable_2_values, \\\n",
    "                           self.env.usable_3_values, \\\n",
    "                           self.env.actions_count])\n",
    "        \n",
    "    \n",
    "    def predict_V(self, episode, method='all-visit'):\n",
    "        T = len(episode)\n",
    "        gamma = self.discount_factor\n",
    "        last_reward = episode[T - 1][2]\n",
    "        visit_set = set()\n",
    "        for ep_num, (s, a, r) in enumerate(episode):\n",
    "            stup = s.get_tuple()\n",
    "            G = (gamma ** (T - 1 - ep_num)) * last_reward * 1.0\n",
    "            k = self.visit_count_V[stup]\n",
    "            mean_old = self.value_function[stup]\n",
    "            if method == 'first-visit':\n",
    "                if not stup in visit_set:\n",
    "                    self.value_function[stup] = (1.0 * ((k * mean_old) + G))/(k + 1) \n",
    "                    self.visit_count_V[stup] = k + 1\n",
    "                    visit_set.add(stup)\n",
    "            else:\n",
    "                self.value_function[stup] = (1.0 * ((k * mean_old) + G))/(k + 1) \n",
    "                self.visit_count_V[stup] = k + 1\n",
    "                \n",
    "    def predict_Q(self, episode, method='all-visit'):\n",
    "        T = len(episode)\n",
    "        gamma = self.discount_factor\n",
    "        last_reward = episode[T - 1][2]\n",
    "        visit_set = set()\n",
    "        for ep_num, (s, a, r) in enumerate(episode):\n",
    "            stup = s.get_tuple()\n",
    "            satup = stup + (a.value,)\n",
    "            G = (gamma ** (T - 1 - ep_num)) * last_reward * 1.0\n",
    "            k = self.visit_count_Q[satup]\n",
    "            mean_old = self.action_value_function[satup]\n",
    "            if method == 'first-visit':\n",
    "                if not satup in visit_set:\n",
    "                    self.action_value_function[satup] = (1.0 * ((k * mean_old) + G))/(k + 1) \n",
    "                    self.visit_count_Q[satup] = k + 1\n",
    "                    visit_set.add(satup)\n",
    "            else:\n",
    "                self.action_value_function[satup] = (1.0 * ((k * mean_old) + G))/(k + 1) \n",
    "                self.visit_count_Q[satup] = k + 1\n",
    "        \n",
    "    \n",
    "    def policy(self, state):\n",
    "        if(state.agent_sum >= 25):\n",
    "            return Action.STICK\n",
    "        else:\n",
    "            return Action.HIT\n",
    "        \n",
    "    def generate_episode(self, es=False):\n",
    "        episode = []\n",
    "        state = self.env.initial_state()\n",
    "#         if(state.is_terminal):\n",
    "#             print('terminal state')\n",
    "#             state._print_state_()\n",
    "        while not state.is_terminal:\n",
    "            \n",
    "            # choose action as per agent's policy\n",
    "            if es:\n",
    "                action = self.take_random_action()\n",
    "                es = False\n",
    "            else:\n",
    "                action = self.policy(state)\n",
    "\n",
    "            # execute action in the env and gather rewards\n",
    "            next_state, reward = self.env.step(copy.deepcopy(state), action)\n",
    "\n",
    "            # store the episode\n",
    "            episode.append((copy.deepcopy(state), action, reward))\n",
    "\n",
    "            # update the state\n",
    "            state = next_state\n",
    "            \n",
    "        return episode\n",
    "    \n",
    "    def train_V(self, num_episodes, method='all-visit'):\n",
    "        epi_num = 0\n",
    "        pbar = tqdm(total=num_episodes)\n",
    "        while epi_num < num_episodes:\n",
    "            episode = self.generate_episode(es=False)\n",
    "            if(len(episode) == 0):\n",
    "                continue\n",
    "            self.num_games_played += 1.0\n",
    "            self.predict_V(episode, method)\n",
    "#             if epi_num % 100 == 0 and epi_num != 0:\n",
    "#                 print(\"Episode: %d\" % epi_num)\n",
    "            pbar.update(1)\n",
    "            epi_num += 1\n",
    "        pbar.close()\n",
    "        return self.value_function\n",
    "    \n",
    "    def train_Q(self, num_episodes, method='all-visit'):\n",
    "        epi_num = 0\n",
    "        pbar = tqdm(total=num_episodes)\n",
    "        while epi_num < num_episodes:\n",
    "            episode = self.generate_episode(es=True)\n",
    "            if(len(episode) == 0):\n",
    "                continue\n",
    "            self.num_games_played += 1.0\n",
    "            self.predict_Q(episode, method)\n",
    "#             if epi_num % 100 == 0 and epi_num != 0:\n",
    "#                 print(\"Episode: %d\" % epi_num)\n",
    "            pbar.update(1)\n",
    "            epi_num += 1\n",
    "        pbar.close()\n",
    "        return self.action_value_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_MC(num_episodes, method):\n",
    "    environment = Environment()\n",
    "    mc_agent = MCAgentEvaluation(environment)\n",
    "    V = mc_agent.train_V(num_episodes, method)\n",
    "    for i in range(0,3):\n",
    "        for j in range(0,3):\n",
    "            for k in range(0,3):\n",
    "                save_path = './MC/' + str(num_episodes) + '-' + str(i) + '-' + str(j) + '-' + str(k) + '-' + method + '.png'\n",
    "                plot_value_function('MC', V, method=method, train_steps=num_episodes, usables=[i,j,k], save=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = [100, 100000, 1000000, 10000000]\n",
    "methods = ['all-visit', 'first-visit']\n",
    "for count in counts:\n",
    "    for method in methods:\n",
    "        train_and_save_MC(count, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = Environment()\n",
    "mc_agent = MCAgentEvaluation(environment)\n",
    "V = mc_agent.train_V(100000, 'all-visit')\n",
    "plot_value_function('MC', V, method='all-visit', train_steps=100000, usables=[0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = Environment()\n",
    "mc_agent = MCAgentEvaluation(environment)\n",
    "num_episodes = 100000\n",
    "method = 'all-visit'\n",
    "V = mc_agent.train_V(num_episodes, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_value_function(mc_agent.get_value_function(), method=method, train_steps=num_episodes, usables=[0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_Q = mc_agent.train_Q(1000, 'first-visit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Step TD Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kTDEvaluation(Agent):\n",
    "    def __init__(self, environment, discount_factor=1, alpha=0.1, k=1):\n",
    "        Agent.__init__(self, environment, discount_factor)\n",
    "        self.alpha =  alpha\n",
    "        self.k = k\n",
    "    \n",
    "    def policy(self, state):\n",
    "        if(state.agent_sum >= 25):\n",
    "            return Action.STICK\n",
    "        else:\n",
    "            return Action.HIT\n",
    "        \n",
    "    def train_V(self, num_episodes=100):\n",
    "        pbar = tqdm(total=num_episodes)\n",
    "        for i in range(num_episodes):\n",
    "            \n",
    "            # get a non-terminal state S0\n",
    "            state = self.env.initial_state()\n",
    "            while(state.is_terminal):\n",
    "                state = self.env.initial_state()\n",
    "\n",
    "            # store_states\n",
    "            states_seen = {}\n",
    "            states_seen[0] = state.get_tuple()\n",
    "\n",
    "            # initialise T <- infinity\n",
    "            T = float('inf')\n",
    "            t = 0\n",
    "            last_reward = 0\n",
    "            k = self.k\n",
    "            tau = 1 - k\n",
    "            gamma = self.discount_factor\n",
    "            alpha = self.alpha\n",
    "    \n",
    "            while(tau != (T - 1)):\n",
    "                if t < T:\n",
    "                    if not state.is_terminal:\n",
    "                        action = self.policy(state)\n",
    "                        next_state, reward = self.env.step(copy.deepcopy(state), action)\n",
    "                        states_seen[t + 1] = next_state.get_tuple()\n",
    "                        state = next_state\n",
    "                    if(next_state.is_terminal):\n",
    "                        T = t + 1\n",
    "                        last_reward = reward\n",
    "                tau = t - k + 1\n",
    "                if tau >= 0:\n",
    "                    G = 0\n",
    "                    if T != float('inf'):\n",
    "                        G = ((gamma ** (T - tau - 1)) * last_reward)\n",
    "                    if (tau + k < T):\n",
    "                        stup = states_seen[tau + k]\n",
    "                        G += ((gamma ** k) * self.value_function[stup])\n",
    "                    stup_to_update = states_seen[tau]\n",
    "                    self.value_function[stup_to_update] += (alpha * (G - self.value_function[stup_to_update]))\n",
    "                t += 1\n",
    "            pbar.update(1)\n",
    "        pbar.close()\n",
    "        return self.value_function\n",
    "    \n",
    "    def train_Q(self, num_episodes=100):\n",
    "        pbar = tqdm(total=num_episodes)\n",
    "        for i in range(num_episodes):\n",
    "            \n",
    "            # get a non-terminal state S0\n",
    "            state = self.env.initial_state()\n",
    "            while(state.is_terminal):\n",
    "                state = self.env.initial_state()\n",
    "\n",
    "            # store_states\n",
    "            states_seen = {}\n",
    "            action = self.take_random_action()\n",
    "            states_seen[0] = state.get_tuple() + (action.value,)\n",
    "\n",
    "            # initialise T <- infinity\n",
    "            T = float('inf')\n",
    "            t = 0\n",
    "            last_reward = 0\n",
    "            k = self.k\n",
    "            tau = 1 - k\n",
    "            gamma = self.discount_factor\n",
    "            alpha = self.alpha\n",
    "    \n",
    "            while(tau != (T - 1)):\n",
    "                if t < T:\n",
    "                    next_state, reward = self.env.step(copy.deepcopy(state), action)\n",
    "                    if not next_state.is_terminal:\n",
    "                        next_action = self.policy(next_state)\n",
    "                        satup = next_state.get_tuple() + (next_action.value,)\n",
    "                        states_seen[t + 1] = satup\n",
    "                        state = next_state\n",
    "                        action = next_action\n",
    "                    if(next_state.is_terminal):\n",
    "                        T = t + 1\n",
    "                        last_reward = reward\n",
    "                tau = t - k + 1\n",
    "                if tau >= 0:\n",
    "                    G = 0\n",
    "                    if T != float('inf'):\n",
    "                        G = ((gamma ** (T - tau - 1)) * last_reward)\n",
    "                    if (tau + k < T):\n",
    "                        stup = states_seen[tau + k]\n",
    "                        G += ((gamma ** k) * self.action_value_function[stup])\n",
    "                    stup_to_update = states_seen[tau]\n",
    "                    self.action_value_function[stup_to_update] += (alpha * (G - self.action_value_function[stup_to_update]))\n",
    "                t += 1\n",
    "            pbar.update(1)\n",
    "        pbar.close()\n",
    "        return self.action_value_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_TD(num_episodes, k, save=False):\n",
    "    environment = Environment()\n",
    "    td_agent = kTDEvaluation(environment, alpha=0.1, k=k)\n",
    "    V = td_agent.train_V(num_episodes)\n",
    "    if save:\n",
    "        for i in range(0,3):\n",
    "            for j in range(0,3):\n",
    "                for kk in range(0,3):\n",
    "                    save_path = './TD/' + str(k) + '-' + str(num_episodes) + '-' + str(i) + '-' + str(j) + '-' + str(kk) + '.png'\n",
    "                    plot_value_function('TD', V, method='k='+str(k), train_steps=num_episodes, usables=[i,j,kk], save=save_path)\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_episodes = [25, 250]\n",
    "ks = [1, 3, 5, 10, 100, 1000]\n",
    "num_episodes = 5000\n",
    "for k in ks:\n",
    "    for episode_count in all_episodes:\n",
    "        V = np.zeros([32,10,3,3,3])\n",
    "        for i in range(episode_count):\n",
    "            V += train_and_save_TD(num_episodes, k, False)\n",
    "            final_V = V / episode_count\n",
    "        for i in range(0,3):\n",
    "            for j in range(0,3):\n",
    "                for kk in range(0,3):\n",
    "                    save_path = './TD/' + 'average-' + str(k) + '-' + str(episode_count) + '-' + str(num_episodes) + '-' + str(i) + '-' + str(j) + '-' + str(kk) + '.png'\n",
    "                    plot_value_function('TD', final_V, method='k='+str(k), train_steps=10000, usables=[i,j,kk], save=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = Environment()\n",
    "num_episodes = 10000\n",
    "k = 3\n",
    "td_agent = kTDEvaluation(environment, alpha=0.1, k=k)\n",
    "V = td_agent.train_V(num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stup = (0,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V[stup].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = Environment()\n",
    "num_episodes = 10000\n",
    "k = 3\n",
    "td_agent = kTDEvaluation(environment, alpha=0.1, k=k)\n",
    "Q = td_agent.train_Q(num_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V[:,:,0,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-step SARSA Agent (Fixed or Decaying Epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kSarsaControl(Agent):\n",
    "    def __init__(self, environment, alpha=0.1, k=1, discount_factor=1):\n",
    "        Agent.__init__(self, environment, discount_factor)\n",
    "        self.alpha =  alpha\n",
    "        self.k = k\n",
    "    \n",
    "    def ep_greedy_policy(self, state, epsilon):\n",
    "        num = random.random()\n",
    "        stup = state.get_tuple()\n",
    "        Q = self.get_action_value_function()\n",
    "        if(num <= epsilon):\n",
    "            action = self.take_random_action()\n",
    "        else:\n",
    "            action = Action(np.argmax(Q[stup]))\n",
    "        return action\n",
    "    \n",
    "    def policy(self, state):\n",
    "        return self.take_greedy_action(state)\n",
    "    \n",
    "    def generate_test_episode(self, epsilon=0.1, es=True):\n",
    "        episode = []\n",
    "        \n",
    "        state = self.env.initial_state()\n",
    "        while(state.is_terminal):\n",
    "            state = self.env.initial_state()\n",
    "        \n",
    "        while not state.is_terminal:\n",
    "            \n",
    "            # choose action as per agent's policy\n",
    "            action = self.policy(state)\n",
    "\n",
    "            # execute action in the env and gather rewards\n",
    "            next_state, reward = self.env.step(copy.deepcopy(state), action)\n",
    "\n",
    "            # store the episode\n",
    "            episode.append((copy.deepcopy(state), action, reward))\n",
    "\n",
    "            # update the state\n",
    "            state = next_state\n",
    "            \n",
    "        return episode\n",
    "    \n",
    "    \n",
    "    def train_Q(self, num_episodes=100, epsilon=0.1, decay=False, get_total_reward=False):\n",
    "        pbar = tqdm(total=num_episodes)\n",
    "        total_reward = []\n",
    "        for i in range(num_episodes):\n",
    "            \n",
    "            # get a non-terminal state S0\n",
    "            state = self.env.initial_state()\n",
    "            while(state.is_terminal):\n",
    "                state = self.env.initial_state()\n",
    "\n",
    "            # store_states\n",
    "            states_seen = {}\n",
    "            action = self.take_random_action()\n",
    "            states_seen[0] = state.get_tuple() + (action.value,)\n",
    "\n",
    "            # initialise T <- infinity\n",
    "            T = float('inf')\n",
    "            t = 0\n",
    "            last_reward = 0\n",
    "            k = self.k\n",
    "            tau = 1 - k\n",
    "            gamma = self.discount_factor\n",
    "            alpha = self.alpha\n",
    "            count = 1\n",
    "    \n",
    "            while(tau != (T - 1)):\n",
    "                if t < T:\n",
    "                    next_state, reward = self.env.step(copy.deepcopy(state), action)\n",
    "                    if not next_state.is_terminal:\n",
    "                        next_action = self.ep_greedy_policy(next_state, ((1.0 * epsilon)/count))\n",
    "                        satup = next_state.get_tuple() + (next_action.value,)\n",
    "                        states_seen[t + 1] = satup\n",
    "                        state = next_state\n",
    "                        action = next_action\n",
    "                    if(next_state.is_terminal):\n",
    "                        T = t + 1\n",
    "                        last_reward = reward\n",
    "                tau = t - k + 1\n",
    "                if tau >= 0:\n",
    "                    G = 0\n",
    "                    if T != float('inf'):\n",
    "                        G = ((gamma ** (T - tau - 1)) * last_reward)\n",
    "                    if (tau + k < T):\n",
    "                        stup = states_seen[tau + k]\n",
    "                        G += ((gamma ** k) * self.action_value_function[stup])\n",
    "                    stup_to_update = states_seen[tau]\n",
    "                    self.action_value_function[stup_to_update] += (alpha * (G - self.action_value_function[stup_to_update]))\n",
    "                    if decay:\n",
    "                        count += 1\n",
    "                t += 1\n",
    "            total_reward += [last_reward]\n",
    "            pbar.update(1)\n",
    "        pbar.close()\n",
    "        if get_total_reward:\n",
    "            return self.get_action_value_function(), total_reward\n",
    "        return self.get_action_value_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "environment = Environment()\n",
    "num_episodes = 100000\n",
    "k = 10\n",
    "sarsa_agent = kSarsaControl(environment, alpha=0.1, k=k)\n",
    "Q = sarsa_agent.train_Q(num_episodes, decay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_value_function('SARSA', sarsa_agent.get_value_function(), method='k='+str(k), train_steps=num_episodes, usables=[0,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent(Agent):\n",
    "    def __init__(self, environment, alpha=0.1, discount_factor=1):\n",
    "        Agent.__init__(self, environment, discount_factor)\n",
    "        self.alpha =  alpha\n",
    "        self.k = 1\n",
    "    \n",
    "    def ep_greedy_policy(self, state, epsilon):\n",
    "        num = random.random()\n",
    "        stup = state.get_tuple()\n",
    "        Q = self.get_action_value_function()\n",
    "        if(num <= epsilon):\n",
    "            action = self.take_random_action()\n",
    "        else:\n",
    "            action = Action(np.argmax(Q[stup]))\n",
    "        return action\n",
    "    \n",
    "    def policy(self, state):\n",
    "        return self.take_greedy_action(state)\n",
    "    \n",
    "    def generate_test_episode(self, epsilon=0.1, es=True):\n",
    "        episode = []\n",
    "        \n",
    "        state = self.env.initial_state()\n",
    "        while(state.is_terminal):\n",
    "            state = self.env.initial_state()\n",
    "        \n",
    "        while not state.is_terminal:\n",
    "            \n",
    "            # choose action as per agent's policy\n",
    "            action = self.policy(state)\n",
    "\n",
    "            # execute action in the env and gather rewards\n",
    "            next_state, reward = self.env.step(copy.deepcopy(state), action)\n",
    "\n",
    "            # store the episode\n",
    "            episode.append((copy.deepcopy(state), action, reward))\n",
    "\n",
    "            # update the state\n",
    "            state = next_state\n",
    "            \n",
    "        return episode\n",
    "    \n",
    "    def generate_episode(self, epsilon=0.1, decay=False, es=True):\n",
    "        episode = []\n",
    "        \n",
    "        state = self.env.initial_state()\n",
    "        while(state.is_terminal):\n",
    "            state = self.env.initial_state()\n",
    "        \n",
    "        count = 0\n",
    "        while not state.is_terminal:\n",
    "            \n",
    "            # choose action as per agent's policy\n",
    "            if es:\n",
    "                action = self.take_random_action()\n",
    "                es = False\n",
    "            else:\n",
    "                action = self.ep_greedy_policy(state, (1.0 * epsilon)/count)\n",
    "\n",
    "            # execute action in the env and gather rewards\n",
    "            next_state, reward = self.env.step(copy.deepcopy(state), action)\n",
    "\n",
    "            # store the episode\n",
    "            episode.append((copy.deepcopy(state), action, reward))\n",
    "\n",
    "            # update the state\n",
    "            state = next_state\n",
    "            \n",
    "            if decay:\n",
    "                count += 1\n",
    "            \n",
    "        return episode\n",
    "    \n",
    "    def train_Q(self, num_episodes=100, epsilon=0.1, decay=False, get_total_reward=False):\n",
    "        pbar = tqdm(total=num_episodes)\n",
    "        total_reward = []\n",
    "        for i in range(num_episodes):\n",
    "            \n",
    "            # get a non-terminal state S0\n",
    "            state = self.env.initial_state()\n",
    "            while(state.is_terminal):\n",
    "                state = self.env.initial_state()\n",
    "\n",
    "            # store_states\n",
    "            states_seen = {}\n",
    "            action = self.take_random_action()\n",
    "            states_seen[0] = state.get_tuple() + (action.value,)\n",
    "\n",
    "            # initialise T <- infinity\n",
    "            T = float('inf')\n",
    "            t = 0\n",
    "            last_reward = 0\n",
    "            k = self.k\n",
    "            tau = 1 - k\n",
    "            gamma = self.discount_factor\n",
    "            alpha = self.alpha\n",
    "            count = 1\n",
    "    \n",
    "            while(tau != (T - 1)):\n",
    "                if t < T:\n",
    "                    next_state, reward = self.env.step(copy.deepcopy(state), action)\n",
    "                    if not next_state.is_terminal:\n",
    "                        next_action = self.ep_greedy_policy(next_state, ((1.0 * epsilon)/count))\n",
    "                        satup = next_state.get_tuple() + (next_action.value,)\n",
    "                        states_seen[t + 1] = satup\n",
    "                        state = next_state\n",
    "                        action = next_action\n",
    "                    if(next_state.is_terminal):\n",
    "                        T = t + 1\n",
    "                        last_reward = reward\n",
    "                tau = t - k + 1\n",
    "                if tau >= 0:\n",
    "                    G = 0\n",
    "                    if T != float('inf'):\n",
    "                        G = ((gamma ** (T - tau - 1)) * last_reward)\n",
    "                    if (tau + k < T):\n",
    "                        stup = states_seen[tau + k]\n",
    "                        state_tuple = stup[:-1]\n",
    "                        # maximum of all the qvalues in that state\n",
    "                        max_q_value = np.max(self.action_value_function[state_tuple])\n",
    "                        G += ((gamma ** k) * max_q_value)\n",
    "                    stup_to_update = states_seen[tau]\n",
    "                    self.action_value_function[stup_to_update] += (alpha * (G - self.action_value_function[stup_to_update]))\n",
    "                    if decay:\n",
    "                        count += 1\n",
    "                t += 1\n",
    "            pbar.update(1)\n",
    "            total_reward += [last_reward]\n",
    "        pbar.close()\n",
    "        if get_total_reward:\n",
    "            return self.get_action_value_function(), total_reward\n",
    "        return self.get_action_value_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = Environment()\n",
    "num_episodes = 500000\n",
    "qlearning_agent = QLearningAgent(environment, alpha=0.1)\n",
    "Q = qlearning_agent.train_Q(num_episodes, decay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_value_function('Q Learning', qlearning_agent.get_value_function(), method='k='+str(1), train_steps=num_episodes, usables=[0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward View Eligibility Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sarsa_lambda(Agent):\n",
    "    def __init__(self, environment, alpha=0.1, discount_factor=1, _lambda=0.5):\n",
    "        Agent.__init__(self, environment, discount_factor)\n",
    "        self.alpha =  alpha\n",
    "        self._lambda = _lambda\n",
    "        # randomly initialised\n",
    "        self.action_value_function = np.random.uniform(low=-1.0, high=1.0, size=\n",
    "                                              [self.env.agent_max_value, \\\n",
    "                                              self.env.dealer_max_value, \\\n",
    "                                              self.env.usable_1_values, \\\n",
    "                                              self.env.usable_2_values, \\\n",
    "                                              self.env.usable_3_values, \\\n",
    "                                              self.env.actions_count])\n",
    "        self.E = np.zeros([self.env.agent_max_value, \\\n",
    "                          self.env.dealer_max_value, \\\n",
    "                          self.env.usable_1_values, \\\n",
    "                          self.env.usable_2_values, \\\n",
    "                          self.env.usable_3_values, \\\n",
    "                          self.env.actions_count])\n",
    "        \n",
    "    def ep_greedy_policy(self, state, epsilon):\n",
    "        num = random.random()\n",
    "        stup = state.get_tuple()\n",
    "        Q = self.get_action_value_function()\n",
    "        if(num <= epsilon):\n",
    "            action = self.take_random_action()\n",
    "        else:\n",
    "            action = Action(np.argmax(Q[stup]))\n",
    "        return action\n",
    "    \n",
    "    def random_policy(self, state):\n",
    "        return self.take_random_action()\n",
    "    \n",
    "    def policy(self, state):\n",
    "        return self.take_greedy_action(state)\n",
    "    \n",
    "    def get_state_action_tuple(self, state, action):\n",
    "        stup = state.get_tuple()\n",
    "        return (stup + (action.value, ))\n",
    "    \n",
    "    def generate_test_episode(self, epsilon=0.1, es=True):\n",
    "        episode = []\n",
    "        \n",
    "        state = self.env.initial_state()\n",
    "        while(state.is_terminal):\n",
    "            state = self.env.initial_state()\n",
    "        \n",
    "        while not state.is_terminal:\n",
    "            \n",
    "            # choose action as per agent's policy\n",
    "            action = self.policy(state)\n",
    "\n",
    "            # execute action in the env and gather rewards\n",
    "            next_state, reward = self.env.step(copy.deepcopy(state), action)\n",
    "\n",
    "            # store the episode\n",
    "            episode.append((copy.deepcopy(state), action, reward))\n",
    "\n",
    "            # update the state\n",
    "            state = next_state\n",
    "            \n",
    "        return episode\n",
    "    \n",
    "    def generate_episode(self, epsilon=0.1, decay=True, es=True):\n",
    "        episode = []\n",
    "        \n",
    "        state = self.env.initial_state()\n",
    "        while(state.is_terminal):\n",
    "            state = self.env.initial_state()\n",
    "        \n",
    "        count = 0\n",
    "        while not state.is_terminal:\n",
    "            \n",
    "            # choose action as per agent's policy\n",
    "            if es:\n",
    "                action = self.take_random_action()\n",
    "                es = False\n",
    "            else:\n",
    "                action = self.ep_greedy_policy(state, (1.0 * epsilon)/count)\n",
    "\n",
    "            # execute action in the env and gather rewards\n",
    "            next_state, reward = self.env.step(copy.deepcopy(state), action)\n",
    "\n",
    "            # store the episode\n",
    "            episode.append((copy.deepcopy(state), action, reward))\n",
    "\n",
    "            # update the state\n",
    "            state = next_state\n",
    "            \n",
    "            if decay:\n",
    "                count += 1\n",
    "            \n",
    "        return episode\n",
    "        \n",
    "    \n",
    "    def train_Q_forward(self, num_episodes, epsilon=0.1, decay=True):\n",
    "        pbar = tqdm(total=num_episodes)\n",
    "        for i in range(num_episodes):\n",
    "            gamma = self.discount_factor\n",
    "            alpha = self.alpha\n",
    "            _lambda = self._lambda\n",
    "            episode = self.generate_episode(epsilon, decay, es=True)\n",
    "            T = len(episode)\n",
    "            last_reward = episode[T - 1][2]\n",
    "            for t, (s_t, a_t, _) in enumerate(episode):\n",
    "                G_t_lambda = 0\n",
    "                for k in range(1, T - t):\n",
    "                    state, action, _ = episode[t + k]\n",
    "                    stup = self.get_state_action_tuple(state, action)\n",
    "                    if (t + k < T - 1):\n",
    "                        G_t_k = (gamma ** k) * self.action_value_function[stup]\n",
    "                    else:\n",
    "                        gamma_pow = gamma ** (T - k - 2)\n",
    "                        G_t_k = (gamma_pow * last_reward) + ((gamma_pow * gamma) * self.action_value_function[stup])\n",
    "                    G_t_lambda += ((_lambda ** (k - 1)) * G_t_k)\n",
    "                stup = self.get_state_action_tuple(s_t, a_t)\n",
    "                self.action_value_function[stup] += alpha * (G_t_lambda - self.action_value_function[stup])\n",
    "            pbar.update(1)\n",
    "        pbar.close()\n",
    "        \n",
    "    def train_Q_backward(self, num_episodes, epsilon=0.1, decay=True, get_total_reward=False):\n",
    "        pbar = tqdm(total=num_episodes)\n",
    "        total_reward = []\n",
    "        for i in range(num_episodes):\n",
    "            \n",
    "            # choose a state s\n",
    "            state = self.env.initial_state()\n",
    "            while(state.is_terminal):\n",
    "                state = self.env.initial_state()\n",
    "                \n",
    "            # choose action a from epsilon greedy policy\n",
    "            action = self.ep_greedy_policy(state, epsilon)\n",
    "            next_action = action\n",
    "            \n",
    "            #initialise\n",
    "            gamma = self.discount_factor\n",
    "            _lambda = self._lambda\n",
    "            alpha = self.alpha\n",
    "            count = 2\n",
    "            last_reward = 0\n",
    "            \n",
    "            while not state.is_terminal:\n",
    "                next_state, reward = self.env.step(state, action)\n",
    "                stup = self.get_state_action_tuple(state, action)\n",
    "                if next_state.is_terminal:\n",
    "                    delta = reward - (_lambda * self.action_value_function[stup])\n",
    "                    last_reward = reward\n",
    "                else:\n",
    "                    next_action = self.ep_greedy_policy(next_state, (1.0 * epsilon)/ count)\n",
    "                    next_tup = self.get_state_action_tuple(next_state, next_action)\n",
    "                    delta = reward + ((self.action_value_function[next_tup] - self.action_value_function[stup]) * _lambda)\n",
    "                self.E[stup] += 1\n",
    "                self.action_value_function += (alpha * delta * self.E)\n",
    "                self.E = self.E * (_lambda * gamma)\n",
    "                \n",
    "                state = next_state\n",
    "                action = next_action\n",
    "                if decay:\n",
    "                    count += 1\n",
    "            total_reward += [last_reward]\n",
    "            pbar.update(1)\n",
    "        pbar.close()\n",
    "        if get_total_reward:\n",
    "            return self.get_action_value_function(), total_reward\n",
    "        return self.get_action_value_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = Environment()\n",
    "num_episodes = 5000000\n",
    "sarsa_lambda_agent = Sarsa_lambda(environment, alpha=0.1, _lambda=0.5)\n",
    "Q = sarsa_lambda_agent.train_Q_backward(num_episodes, decay=True, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = sarsa_lambda_agent.train_Q_backward(num_episodes, decay=True, epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_value_function2('Sarsa Lambda', sarsa_lambda_agent.get_value_function(), method='lambda='+str(0.5), train_steps=num_episodes, usables=[2,2,2], transpose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative Rewards v/s Episode Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sarsa_reward(k, decay, num_episodes=100, alpha=0.1, epsilon=0.1):\n",
    "    environment = Environment()\n",
    "    sarsa_agent = kSarsaControl(environment, alpha=alpha, k=k)\n",
    "    _, total_reward = sarsa_agent.train_Q(num_episodes, epsilon=epsilon, decay=decay, get_total_reward=True)\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumu_reward(num_episodes, num_runs = 10):\n",
    "    final_case = np.zeros([10, num_episodes])\n",
    "    for run in range(num_runs):\n",
    "        case1 = []\n",
    "        case2 = []\n",
    "        case3 = []\n",
    "        case4 = []\n",
    "        ks = [1, 10, 100, 1000]\n",
    "        for k in ks:\n",
    "            fixed_ep_reward = get_sarsa_reward(k=k, decay=False, num_episodes=num_episodes)\n",
    "            var_ep_reward = get_sarsa_reward(k=k, decay=True, num_episodes=num_episodes)\n",
    "            case1.append(fixed_ep_reward)\n",
    "            case2.append(var_ep_reward)\n",
    "        qlearning_agent = QLearningAgent(Environment(), alpha=0.1)\n",
    "        _, total_reward = qlearning_agent.train_Q(num_episodes, decay=False, epsilon=0.1, get_total_reward=True)\n",
    "        case3.append(total_reward)\n",
    "        sarsa_lambda_agent = Sarsa_lambda(Environment(), alpha=0.1, _lambda=0.5)\n",
    "        _, total_reward = sarsa_lambda_agent.train_Q_backward(num_episodes, decay=True, epsilon=0.1, get_total_reward=True)\n",
    "        case4.append(total_reward)\n",
    "        final_case += np.vstack((case1, case2, case3, case4))\n",
    "    return ((1.0 * final_case) / num_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumu_rewards_2(num_episodes=100, num_runs=10):\n",
    "    final_case = np.zeros([10, num_episodes])\n",
    "    for run in range(num_runs):\n",
    "        algos = [[] for i in range(10)]\n",
    "        for epi_count in range(1, num_episodes + 1):\n",
    "            ks = [1, 10, 100, 1000]\n",
    "            for i, k in enumerate(ks):\n",
    "                fixed_ep_reward = get_sarsa_reward(k=k, decay=False, num_episodes=epi_count)\n",
    "                var_ep_reward = get_sarsa_reward(k=k, decay=True, num_episodes=epi_count)\n",
    "                algos[i].append(np.average(fixed_ep_reward))\n",
    "                algos[i + 4].append(np.average(var_ep_reward))\n",
    "            qlearning_agent = QLearningAgent(Environment(), alpha=0.1)\n",
    "            _, total_reward = qlearning_agent.train_Q(epi_count, decay=False, epsilon=0.1, get_total_reward=True)\n",
    "            algos[8].append(np.average(total_reward))\n",
    "            sarsa_lambda_agent = Sarsa_lambda(Environment(), alpha=0.1, _lambda=0.5)\n",
    "            _, total_reward = sarsa_lambda_agent.train_Q_backward(epi_count, decay=True, epsilon=0.1, get_total_reward=True)\n",
    "            algos[9].append(np.average(total_reward))\n",
    "        final_case += np.asarray(algos)\n",
    "    return ((1.0 * final_case) / num_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_rewards = cumu_rewards_2(num_episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_names = {0:'sarsa k=1',\n",
    "             1:'sarsa k=10',\n",
    "             2:'sarsa k=100',\n",
    "             3:'sarsa k=1000',\n",
    "             4: 'sarsa k=1 decay',\n",
    "             5:'sarsa k=10 decay',\n",
    "             6:'sarsa k=100 decay',\n",
    "             7:'sarsa k=1000 decay',\n",
    "             8: 'Q learning',\n",
    "             9: 'TD lambda'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(21,10))\n",
    "compare_list = [3, 7, 8, 9]\n",
    "for i in compare_list:\n",
    "    plt.plot(np.arange(100), avg_rewards[i], label=algo_names[i])\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance variation with Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(agent, test_count=10):\n",
    "    rewards = []\n",
    "    for ep in range(test_count):\n",
    "        episode = agent.generate_test_episode()\n",
    "        T = len(episode)\n",
    "        reward = episode[T - 1][2]\n",
    "        rewards.append(reward)\n",
    "    return np.average(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "num_episodes = 100000\n",
    "ks = [1, 10, 100, 1000]\n",
    "all_perfs = []\n",
    "for alpha in alphas:\n",
    "    perfs = []\n",
    "    for k in ks:\n",
    "        sarsa_agent = kSarsaControl(Environment(), alpha=alpha, k=k)\n",
    "        Q = sarsa_agent.train_Q(num_episodes, epsilon=0.1, decay=False)\n",
    "        perf = generate_episode(agent=sarsa_agent, test_count=10)\n",
    "        perfs.append(perf)\n",
    "    for k in ks:\n",
    "        sarsa_agent = kSarsaControl(Environment(), alpha=alpha, k=k)\n",
    "        Q = sarsa_agent.train_Q(num_episodes, epsilon=0.1, decay=True)\n",
    "        perf = generate_episode(agent=sarsa_agent, test_count=10)\n",
    "        perfs.append(perf)\n",
    "    qlearning_agent = QLearningAgent(Environment(), alpha=alpha)\n",
    "    Q = qlearning_agent.train_Q(num_episodes, decay=False, epsilon=0.1)\n",
    "    perf = generate_episode(agent=qlearning_agent, test_count=10)\n",
    "    perfs.append(perf)\n",
    "    sarsa_lambda_agent = Sarsa_lambda(Environment(), alpha=0.1, _lambda=0.5)\n",
    "    Q = sarsa_lambda_agent.train_Q_backward(num_episodes, decay=True, epsilon=0.1)\n",
    "    perf = generate_episode(agent=sarsa_lambda_agent, test_count=10)\n",
    "    perfs.append(perf)\n",
    "    all_perfs.append(perfs)\n",
    "all_perfs = np.asarray(all_perfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_perfs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_names = {0:'sarsa k=1',\n",
    "             1:'sarsa k=10',\n",
    "             2:'sarsa k=100',\n",
    "             3:'sarsa k=1000',\n",
    "             4: 'sarsa k=1 decay',\n",
    "             5:'sarsa k=10 decay',\n",
    "             6:'sarsa k=100 decay',\n",
    "             7:'sarsa k=1000 decay',\n",
    "             8: 'Q learning',\n",
    "             9: 'TD lambda'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(21,10))\n",
    "compare_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "for i in compare_list:\n",
    "    plt.plot(alphas, all_perfs[:, i], label=algo_names[i], marker='o')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('average reward per episode')\n",
    "plt.title('Variation of performance with learning rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(21,10))\n",
    "compare_list = [0, 1, 2, 3, 4]\n",
    "x = list(algo_names.keys())\n",
    "y = list(algo_names.values())\n",
    "for i in range(len(alphas)):\n",
    "    plt.plot(y, all_perfs[i, :], label = alphas[i], marker='o')\n",
    "plt.xticks(x, y)\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('agent type')\n",
    "plt.ylabel('average reward per episode')\n",
    "plt.title('Choosing appropriate learning rate for each agent')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
